{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: dask in c:\\users\\atul1\\work\\tools\\python3_12\\lib\\site-packages (2025.2.0)\n",
      "Requirement already satisfied: click>=8.1 in c:\\users\\atul1\\work\\tools\\python3_12\\lib\\site-packages (from dask) (8.1.8)\n",
      "Requirement already satisfied: cloudpickle>=3.0.0 in c:\\users\\atul1\\work\\tools\\python3_12\\lib\\site-packages (from dask) (3.1.1)\n",
      "Requirement already satisfied: fsspec>=2021.09.0 in c:\\users\\atul1\\work\\tools\\python3_12\\lib\\site-packages (from dask) (2025.2.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\atul1\\appdata\\roaming\\python\\python312\\site-packages (from dask) (24.2)\n",
      "Requirement already satisfied: partd>=1.4.0 in c:\\users\\atul1\\work\\tools\\python3_12\\lib\\site-packages (from dask) (1.4.2)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in c:\\users\\atul1\\work\\tools\\python3_12\\lib\\site-packages (from dask) (6.0.2)\n",
      "Requirement already satisfied: toolz>=0.10.0 in c:\\users\\atul1\\work\\tools\\python3_12\\lib\\site-packages (from dask) (1.0.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\atul1\\work\\tools\\python3_12\\lib\\site-packages (from click>=8.1->dask) (0.4.6)\n",
      "Requirement already satisfied: locket in c:\\users\\atul1\\work\\tools\\python3_12\\lib\\site-packages (from partd>=1.4.0->dask) (1.0.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\atul1\\work\\tools\\python3_12\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: pandas in c:\\users\\atul1\\work\\tools\\python3_12\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\atul1\\work\\tools\\python3_12\\lib\\site-packages (from pandas) (2.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\atul1\\work\\tools\\python3_12\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\atul1\\work\\tools\\python3_12\\lib\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\atul1\\work\\tools\\python3_12\\lib\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\atul1\\work\\tools\\python3_12\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: pyarrow in c:\\users\\atul1\\work\\tools\\python3_12\\lib\\site-packages (19.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install dask\n",
    "!pip install numpy\n",
    "!pip install pandas\n",
    "!pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: plotly in c:\\users\\atul1\\work\\tools\\python3_12\\lib\\site-packages (6.0.0)\n",
      "Requirement already satisfied: narwhals>=1.15.1 in c:\\users\\atul1\\work\\tools\\python3_12\\lib\\site-packages (from plotly) (1.27.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\atul1\\appdata\\roaming\\python\\python312\\site-packages (from plotly) (24.2)\n",
      "Collecting nbformat\n",
      "  Downloading nbformat-5.10.4-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting fastjsonschema>=2.15 (from nbformat)\n",
      "  Downloading fastjsonschema-2.21.1-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting jsonschema>=2.6 (from nbformat)\n",
      "  Downloading jsonschema-4.23.0-py3-none-any.whl.metadata (7.9 kB)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in c:\\users\\atul1\\appdata\\roaming\\python\\python312\\site-packages (from nbformat) (5.7.2)\n",
      "Requirement already satisfied: traitlets>=5.1 in c:\\users\\atul1\\appdata\\roaming\\python\\python312\\site-packages (from nbformat) (5.14.3)\n",
      "Collecting attrs>=22.2.0 (from jsonschema>=2.6->nbformat)\n",
      "  Downloading attrs-25.1.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting jsonschema-specifications>=2023.03.6 (from jsonschema>=2.6->nbformat)\n",
      "  Downloading jsonschema_specifications-2024.10.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting referencing>=0.28.4 (from jsonschema>=2.6->nbformat)\n",
      "  Downloading referencing-0.36.2-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting rpds-py>=0.7.1 (from jsonschema>=2.6->nbformat)\n",
      "  Downloading rpds_py-0.22.3-cp312-cp312-win_amd64.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: platformdirs>=2.5 in c:\\users\\atul1\\appdata\\roaming\\python\\python312\\site-packages (from jupyter-core!=5.0.*,>=4.12->nbformat) (4.3.6)\n",
      "Requirement already satisfied: pywin32>=300 in c:\\users\\atul1\\appdata\\roaming\\python\\python312\\site-packages (from jupyter-core!=5.0.*,>=4.12->nbformat) (308)\n",
      "Collecting typing-extensions>=4.4.0 (from referencing>=0.28.4->jsonschema>=2.6->nbformat)\n",
      "  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Downloading nbformat-5.10.4-py3-none-any.whl (78 kB)\n",
      "Downloading fastjsonschema-2.21.1-py3-none-any.whl (23 kB)\n",
      "Downloading jsonschema-4.23.0-py3-none-any.whl (88 kB)\n",
      "Downloading attrs-25.1.0-py3-none-any.whl (63 kB)\n",
      "Downloading jsonschema_specifications-2024.10.1-py3-none-any.whl (18 kB)\n",
      "Downloading referencing-0.36.2-py3-none-any.whl (26 kB)\n",
      "Downloading rpds_py-0.22.3-cp312-cp312-win_amd64.whl (235 kB)\n",
      "Downloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Installing collected packages: fastjsonschema, typing-extensions, rpds-py, attrs, referencing, jsonschema-specifications, jsonschema, nbformat\n",
      "Successfully installed attrs-25.1.0 fastjsonschema-2.21.1 jsonschema-4.23.0 jsonschema-specifications-2024.10.1 nbformat-5.10.4 referencing-0.36.2 rpds-py-0.22.3 typing-extensions-4.12.2\n"
     ]
    }
   ],
   "source": [
    "!pip install plotly\n",
    "!pip install nbformat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: nbformat\n",
      "Version: 5.10.4\n",
      "Summary: The Jupyter Notebook format\n",
      "Home-page: https://jupyter.org\n",
      "Author: \n",
      "Author-email: Jupyter Development Team <jupyter@googlegroups.com>\n",
      "License: BSD 3-Clause License\n",
      "\n",
      "- Copyright (c) 2001-2015, IPython Development Team\n",
      "- Copyright (c) 2015-, Jupyter Development Team\n",
      "\n",
      "All rights reserved.\n",
      "\n",
      "Redistribution and use in source and binary forms, with or without\n",
      "modification, are permitted provided that the following conditions are met:\n",
      "\n",
      "1. Redistributions of source code must retain the above copyright notice, this\n",
      "   list of conditions and the following disclaimer.\n",
      "\n",
      "2. Redistributions in binary form must reproduce the above copyright notice,\n",
      "   this list of conditions and the following disclaimer in the documentation\n",
      "   and/or other materials provided with the distribution.\n",
      "\n",
      "3. Neither the name of the copyright holder nor the names of its\n",
      "   contributors may be used to endorse or promote products derived from\n",
      "   this software without specific prior written permission.\n",
      "\n",
      "THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n",
      "AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n",
      "IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n",
      "DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\n",
      "FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n",
      "DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n",
      "SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n",
      "CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n",
      "OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n",
      "OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n",
      "Location: C:\\Users\\atul1\\Work\\tools\\Python3_12\\Lib\\site-packages\n",
      "Requires: fastjsonschema, jsonschema, jupyter-core, traitlets\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show nbformat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seaborn in c:\\users\\atul1\\work\\tools\\python3_12\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in c:\\users\\atul1\\work\\tools\\python3_12\\lib\\site-packages (from seaborn) (2.2.3)\n",
      "Requirement already satisfied: pandas>=1.2 in c:\\users\\atul1\\work\\tools\\python3_12\\lib\\site-packages (from seaborn) (2.2.3)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in c:\\users\\atul1\\work\\tools\\python3_12\\lib\\site-packages (from seaborn) (3.10.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\atul1\\work\\tools\\python3_12\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\atul1\\work\\tools\\python3_12\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\atul1\\work\\tools\\python3_12\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\atul1\\work\\tools\\python3_12\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\atul1\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\atul1\\work\\tools\\python3_12\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\atul1\\work\\tools\\python3_12\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\atul1\\work\\tools\\python3_12\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\atul1\\work\\tools\\python3_12\\lib\\site-packages (from pandas>=1.2->seaborn) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\atul1\\work\\tools\\python3_12\\lib\\site-packages (from pandas>=1.2->seaborn) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\atul1\\work\\tools\\python3_12\\lib\\site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\n",
      "Collecting folium\n",
      "  Downloading folium-0.19.4-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting branca>=0.6.0 (from folium)\n",
      "  Downloading branca-0.8.1-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting jinja2>=2.9 (from folium)\n",
      "  Downloading jinja2-3.1.5-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\atul1\\work\\tools\\python3_12\\lib\\site-packages (from folium) (2.2.3)\n",
      "Requirement already satisfied: requests in c:\\users\\atul1\\work\\tools\\python3_12\\lib\\site-packages (from folium) (2.32.3)\n",
      "Collecting xyzservices (from folium)\n",
      "  Downloading xyzservices-2025.1.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2>=2.9->folium)\n",
      "  Downloading MarkupSafe-3.0.2-cp312-cp312-win_amd64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\atul1\\work\\tools\\python3_12\\lib\\site-packages (from requests->folium) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\atul1\\work\\tools\\python3_12\\lib\\site-packages (from requests->folium) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\atul1\\work\\tools\\python3_12\\lib\\site-packages (from requests->folium) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\atul1\\work\\tools\\python3_12\\lib\\site-packages (from requests->folium) (2025.1.31)\n",
      "Downloading folium-0.19.4-py2.py3-none-any.whl (110 kB)\n",
      "Downloading branca-0.8.1-py3-none-any.whl (26 kB)\n",
      "Downloading jinja2-3.1.5-py3-none-any.whl (134 kB)\n",
      "Downloading xyzservices-2025.1.0-py3-none-any.whl (88 kB)\n",
      "Downloading MarkupSafe-3.0.2-cp312-cp312-win_amd64.whl (15 kB)\n",
      "Installing collected packages: xyzservices, MarkupSafe, jinja2, branca, folium\n",
      "Successfully installed MarkupSafe-3.0.2 branca-0.8.1 folium-0.19.4 jinja2-3.1.5 xyzservices-2025.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install seaborn\n",
    "!pip install folium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\atul1\\Work\\tools\\Python3_12\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "import folium\n",
    "from folium.plugins import MarkerCluster\n",
    "import numpy as np\n",
    "import random\n",
    "#https://simplemaps.com/data/us-cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_file_path = r\"..\\data\\US_Accidents_March23.csv\"\n",
    "fl_data_file = r\"..\\data\\state_wise_data\\FL_org.csv\"\n",
    "ca_data_file = r\"..\\data\\state_wise_data\\CA_org.csv\"\n",
    "va_data_file = r\"..\\data\\state_wise_data\\VA_org.csv\"\n",
    "ny_data_file = r\"..\\data\\state_wise_data\\NY_org.csv\"\n",
    "chunk_size = 100000  # Adjust based on available memory\n",
    "raw_data_file_frame = \"\"\n",
    "cleaned_data_file_frame = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>State</th>\n",
       "      <th>Population</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AL</td>\n",
       "      <td>3977628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AK</td>\n",
       "      <td>557899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AZ</td>\n",
       "      <td>5848310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AR</td>\n",
       "      <td>2362124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CA</td>\n",
       "      <td>30519524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CO</td>\n",
       "      <td>4662926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>CT</td>\n",
       "      <td>2894190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>DE</td>\n",
       "      <td>819952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>DC</td>\n",
       "      <td>552380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>FL</td>\n",
       "      <td>18229883</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  State  Population\n",
       "0    AL     3977628\n",
       "1    AK      557899\n",
       "2    AZ     5848310\n",
       "3    AR     2362124\n",
       "4    CA    30519524\n",
       "5    CO     4662926\n",
       "6    CT     2894190\n",
       "7    DE      819952\n",
       "8    DC      552380\n",
       "9    FL    18229883"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Creating a DataFrame from the given data\n",
    "# https://www.census.gov/data/tables/time-series/demo/popest/2020s-state-detail.html?utm_source=chatgpt.com\n",
    "USA_2023_POPULATION_DATA = {\n",
    "    \"State\": [\n",
    "        \"AL\", \"AK\", \"AZ\", \"AR\", \"CA\", \"CO\", \"CT\", \"DE\", \"DC\", \"FL\", \"GA\", \"HI\", \"ID\", \"IL\", \"IN\", \"IA\", \"KS\", \"KY\",\n",
    "        \"LA\", \"ME\", \"MD\", \"MA\", \"MI\", \"MN\", \"MS\", \"MO\", \"MT\", \"NE\", \"NV\", \"NH\", \"NJ\", \"NM\", \"NY\", \"NC\", \"ND\", \"OH\",\n",
    "        \"OK\", \"OR\", \"PA\", \"RI\", \"SC\", \"SD\", \"TN\", \"TX\", \"UT\", \"VT\", \"VA\", \"WA\", \"WV\", \"WI\", \"WY\"\n",
    "    ],\n",
    "    \"Population\": [\n",
    "        3977628, 557899, 5848310, 2362124, 30519524, 4662926, 2894190, 819952, 552380, 18229883,\n",
    "        8490546, 1141525, 1497384, 9844167, 5274945, 2476882, 2246209, 3509259, 3506600, 1146670,\n",
    "        4818337, 5659598, 7925350, 4436981, 2259864, 4821686, 897161, 1497381, 2508220, 1150004,\n",
    "        7280551, 1663024, 15611308, 8498868, 599192, 9207681, 3087217, 3401528, 10332678, 892124,\n",
    "        4229354, 697420, 5555761, 22942176, 2484582, 532828, 6834154, 6164810, 1417859, 4661826, 454508\n",
    "    ]\n",
    "}\n",
    "\n",
    "usa_2023_population_age_18_and_up_data_frame = pd.DataFrame(USA_2023_POPULATION_DATA)\n",
    "display(usa_2023_population_age_18_and_up_data_frame.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_large_csv(file_path, chunk_size):\n",
    "    \"\"\"Reads a large CSV file in chunks and returns a generator of chunks.\"\"\"\n",
    "    return pd.read_csv(file_path, chunksize=chunk_size, low_memory=False)\n",
    "\n",
    "def process_large_csv(file_path, chunk_size=100000):\n",
    "    \"\"\"Processes the large CSV file and returns the cleaned DataFrame.\"\"\"\n",
    "    df_combined = pd.DataFrame()\n",
    "\n",
    "    for chunk in read_large_csv(file_path, chunk_size):\n",
    "        df_combined = pd.concat([df_combined, chunk], ignore_index=True)\n",
    "\n",
    "    return df_combined\n",
    "\n",
    "\n",
    "def drop_columns(data_frame):\n",
    "    cols_to_drop = ['ID', 'Source', 'End_Lat', 'End_Lng', 'Distance(mi)', \n",
    "                    'Description', 'Airport_Code', 'Pressure(in)', \n",
    "                    'Wind_Direction', 'Precipitation(in)', \n",
    "                    'Amenity','Give_Way', 'No_Exit', 'Traffic_Calming', \n",
    "                    'Turning_Loop', 'Nautical_Twilight', 'Astronomical_Twilight',\n",
    "                    'Civil_Twilight','Airport_Code','Sunrise_Sunset'\n",
    "                    ]\n",
    "    return data_frame.drop(cols_to_drop, axis=1)\n",
    "\n",
    "\n",
    "def split_and_save_by_state(data_frame, output_dir=r\"..\\data\\state_wise_data\"):\n",
    "    \"\"\"Splits data by state and saves it into separate CSV files.\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    for state, state_df in data_frame.groupby(\"State\"):\n",
    "        if (state == 'FL' or state == 'CA' or state == 'VA' or state == 'NY') :\n",
    "            state_file = os.path.join(output_dir, f\"{state}_org.csv\")\n",
    "            state_df.to_csv(state_file, index=False)\n",
    "    print(f\"Data saved for FA, CA, VA, NY \\n \")\n",
    "\n",
    "def save_cleaned_all_state_data(data_frame, output_dir=r\"..\\data\\cleaned_raw_data\"):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    filePath = os.path.join(output_dir, \"cleaned_all_state_data.csv\")\n",
    "\n",
    "    data_frame.to_csv(filePath, index=False)\n",
    "    print(f\"Saved Cleaned all state Data  \\n \")\n",
    "\n",
    "\n",
    "def print_summary(data_frame):\n",
    "    print(\"** Data Frame Info: \\n\")\n",
    "    print(f\"{data_frame.info()}\")\n",
    "    print(\"\\n ** Data Frame description: \\n\")\n",
    "    print(data_frame.describe(include=\"all\"))\n",
    "    print(\"\\n The shape of data is:\",(raw_data_file_frame.shape))\n",
    "    print(\"\\n Top 3 records \\n\")\n",
    "    display(raw_data_file_frame.head(3))\n",
    "    print(\" \\n Unique Source: \", raw_data_file_frame['Source'].unique())\n",
    "\n",
    "def plot_histogram_state_vs_accident_count(data_frame):\n",
    "    \"\"\"Analyzes the state-wise accident count and plots a histogram.\"\"\"\n",
    "    state_counts = data_frame[\"State\"].value_counts()\n",
    "    \n",
    "    print(f\"\\nNumber of unique states: {state_counts.count()}\")\n",
    "\n",
    "    # Plot histogram for state vs accident count\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.gca().get_yaxis().set_major_formatter(plt.FuncFormatter(lambda x, _: f'{int(x):,}'))   \n",
    "    state_counts.plot(kind=\"bar\", color=\"skyblue\", edgecolor=\"black\")\n",
    "    plt.xlabel(\"State\")\n",
    "    plt.ylabel(\"Total Accidents\")\n",
    "    plt.title(\"Total Accidents by State\")\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_histogram_weather_condition_vs_accident_count(data_frame):\n",
    "    \"\"\"Analyzes the state-wise accident count and plots a histogram.\"\"\"\n",
    "    weather_condition = data_frame[\"Weather_Condition\"].value_counts()\n",
    "    \n",
    "    plt.figure(figsize=(25, 20))\n",
    "    plt.gca().get_yaxis().set_major_formatter(plt.FuncFormatter(lambda x, _: f'{int(x):,}'))   \n",
    "    weather_condition.plot(kind=\"bar\", color=\"skyblue\", edgecolor=\"black\")\n",
    "    plt.xlabel(\"Weather\")\n",
    "    plt.ylabel(\"Total Accidents\")\n",
    "    plt.title(\"Total Accidents by weather condition\")\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "    plt.show()\n",
    "\n",
    "def plot_us_map_accident_count(data_frame):\n",
    "    state_counts = data_frame[\"State\"].value_counts().reset_index()\n",
    "    state_counts.columns = [\"State\", \"Accidents\"]\n",
    "\n",
    "    fig = px.choropleth(\n",
    "        state_counts, \n",
    "        locations=\"State\", \n",
    "        locationmode=\"USA-states\", \n",
    "        color=\"Accidents\",\n",
    "        color_continuous_scale=\"reds\",\n",
    "        title=\"Number of US Accidents for each State\"\n",
    "    )\n",
    "    fig.update_layout(geo=dict(scope=\"usa\"))\n",
    "    fig.show()\n",
    "\n",
    "def plot_histogram_street_vs_accident_count(data_frame):\n",
    "    \"\"\"Analyzes the state-wise accident count and plots a histogram.\"\"\"\n",
    "    weather_condition = data_frame[\"Street\"].value_counts()\n",
    "    \n",
    "    plt.figure(figsize=(25, 20))\n",
    "    plt.gca().get_yaxis().set_major_formatter(plt.FuncFormatter(lambda x, _: f'{int(x):,}'))   \n",
    "    weather_condition.plot(kind=\"bar\", color=\"skyblue\", edgecolor=\"black\")\n",
    "    plt.xlabel(\"Street\")\n",
    "    plt.ylabel(\"Total Accidents\")\n",
    "    plt.title(\"Total Accidents by weather condition\")\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "    plt.show()\n",
    "\n",
    "def plot_histogram_top_20_citi_accident_count(data_frame, text=\"\"):\n",
    "\n",
    "    # Get the top 20 cities with the highest number of accidents\n",
    "    top_cities = data_frame[\"City\"].value_counts().sort_values()[-20:].reset_index()\n",
    "    top_cities.columns = [\"city\", \"number_of_accidents\"]\n",
    "\n",
    "    # Plot the bar chart\n",
    "    plt.figure(figsize=(20,7))\n",
    "    sns.barplot(x=\"city\", y=\"number_of_accidents\", hue=\"city\", data=top_cities, palette='Set2', legend=False)  # âœ… FIXED\n",
    "    plt.title(f\"TOP 20 CITIES WITH HIGHEST NUMBER OF ACCIDENTS - {text}\", fontsize=20)\n",
    "    plt.xticks(rotation=40)\n",
    "    plt.show()\n",
    "\n",
    "def create_map(df_loc, zoom=6, tiles='OpenStreetMap'):\n",
    "    \"\"\"\n",
    "    Generate a Folium Map with clustered markers of accident locations.\n",
    "    Automatically centers the map based on available locations in the dataset.\n",
    "    \"\"\"\n",
    "    # Compute the mean latitude and longitude for centering the map\n",
    "    mean_lat = df_loc['Start_Lat'].mean()\n",
    "    mean_lng = df_loc['Start_Lng'].mean()\n",
    "\n",
    "    # Create the map centered on the computed mean location\n",
    "    state_map  = folium.Map(location=[mean_lat, mean_lng], zoom_start=zoom, tiles=tiles)\n",
    "\n",
    "    # Add marker clustering\n",
    "    marker_cluster = MarkerCluster().add_to(state_map)\n",
    "\n",
    "    # Iterate over the DataFrame rows and add each marker to the cluster\n",
    "    for idx, row in df_loc.iterrows():\n",
    "        folium.Marker(\n",
    "            location=[row['Start_Lat'], row['Start_Lng']],\n",
    "            popup=f\"Lat, Lng: {row['Start_Lat']}, {row['Start_Lng']}\"\n",
    "        ).add_to(marker_cluster)\n",
    "\n",
    "    return state_map\n",
    " \n",
    "def plot_stacked_bar_chart(data_frame):\n",
    "    \"\"\"\n",
    "    Plots a stacked bar chart showing the distribution of accident severity across states.\n",
    "    \"\"\"\n",
    "    # Count the number of accidents by state and severity\n",
    "    severity_counts = data_frame.groupby([\"State\", \"Severity\"]).size().unstack(fill_value=0)\n",
    "\n",
    "    # Define colors for different severity levels\n",
    "    severity_colors = {1: \"#66c2a5\", 2: \"#fc8d62\", 3: \"#8da0cb\", 4: \"#e78ac3\"}\n",
    "\n",
    "    # Plot the stacked bar chart\n",
    "    severity_counts.plot(kind=\"bar\", stacked=True, figsize=(14, 6), color=[severity_colors[i] for i in severity_counts.columns])\n",
    "\n",
    "    # Chart formatting\n",
    "    plt.title(\"Accident Severity Distribution Across States\", fontsize=14)\n",
    "    plt.xlabel(\"State\", fontsize=12)\n",
    "    plt.ylabel(\"Number of Accidents\", fontsize=12)\n",
    "    plt.xticks(rotation=90)  # Rotate x-axis labels for readability\n",
    "    plt.legend(title=\"Severity Level\")\n",
    "    plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# accedent_data_frame\n",
    "def plot_stacked_bar_chart(accedent_data_frame, population_data_frame):\n",
    "\n",
    "    # Count accidents per state per severity level\n",
    "    severity_counts = accedent_data_frame.groupby([\"State\", \"Severity\"]).size().unstack(fill_value=0)\n",
    "    \n",
    "    # Merge with population data\n",
    "    merged_df = population_data_frame.merge(severity_counts, on=\"State\", how=\"left\").fillna(0)\n",
    "    \n",
    "    # Define colors for severity levels\n",
    "    severity_colors = {1: \"#a6cee3\", 2: \"#1f78b4\", 3: \"#b2df8a\", 4: \"#33a02c\"}\n",
    "\n",
    "    # Plot stacked bar chart\n",
    "    ax = merged_df.set_index(\"State\")[sorted(severity_counts.columns)].plot(\n",
    "        kind=\"bar\", stacked=True, figsize=(14, 7), color=[severity_colors[s] for s in sorted(severity_counts.columns)]\n",
    "    )\n",
    "\n",
    "    # Labels and title\n",
    "    plt.xlabel(\"State\")\n",
    "    plt.ylabel(\"Number of Accidents\")\n",
    "    plt.title(\"Stacked Bar Chart of Accidents per State by Severity Level\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    \n",
    "    # Add legend\n",
    "    plt.legend(title=\"Severity Level\", loc=\"upper right\")\n",
    "\n",
    "    # Show plot\n",
    "    plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.6)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_bubble_chart(accedent_data_frame, population_data_frame):\n",
    "    # Aggregate accident count and average severity per state\n",
    "    accident_summary = accedent_data_frame.groupby(\"State\").agg(\n",
    "        Accidents=(\"State\", \"count\"),\n",
    "        Avg_Severity=(\"Severity\", \"mean\")\n",
    "    ).reset_index()\n",
    "\n",
    "    # Merge with population data\n",
    "    merged_df = pd.merge(accident_summary, population_data_frame, on=\"State\")\n",
    "\n",
    "    # Normalize population size for better bubble visualization\n",
    "    pop_scaled = merged_df[\"Population\"] / 50000  # Adjusting scale for better visibility\n",
    "\n",
    "    # Create Bubble Chart\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    scatter = plt.scatter(\n",
    "        merged_df[\"Population\"], \n",
    "        merged_df[\"Accidents\"], \n",
    "        s=pop_scaled,  # Bubble size based on population\n",
    "        c=merged_df[\"Avg_Severity\"],  # Color based on severity\n",
    "        cmap=\"Reds\", \n",
    "        alpha=0.7,\n",
    "        edgecolors=\"k\"\n",
    "    )\n",
    "\n",
    "    # Add state labels on each bubble\n",
    "    for i, row in merged_df.iterrows():\n",
    "        plt.text(row[\"Population\"], row[\"Accidents\"], row[\"State\"], fontsize=10, ha='center', va='center', color=\"black\")\n",
    "\n",
    "    # Add color bar for severity\n",
    "    cbar = plt.colorbar(scatter)\n",
    "    cbar.set_label(\"Average Severity (1=Low, 4=High)\")\n",
    "\n",
    "    # Labels and title\n",
    "    plt.xlabel(\"State Population\")\n",
    "    plt.ylabel(\"Number of Accidents\")\n",
    "    plt.title(\"Bubble Chart of Accidents vs. Population (Scaled by Population & Colored by Severity)\")\n",
    "\n",
    "    # Grid for better readability\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "\n",
    "    # Show plot\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_population_vs_accident_percentage(accedent_data_frame, population_data_frame):\n",
    "\n",
    "    # Count total accidents per state and calculate severity average\n",
    "    accident_summary = accedent_data_frame.groupby(\"State\").agg(\n",
    "        Accidents=(\"State\", \"count\"),\n",
    "        Avg_Severity=(\"Severity\", \"mean\")\n",
    "    ).reset_index()\n",
    "\n",
    "    # Merge with population data\n",
    "    merged_df = pd.merge(accident_summary, population_data_frame, on=\"State\")\n",
    "\n",
    "    # Calculate percentage of population involved in accidents\n",
    "    merged_df[\"Accident_Percentage\"] = (merged_df[\"Accidents\"] / merged_df[\"Population\"]) * 100\n",
    "\n",
    "    # Create Scatter Plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    scatter = plt.scatter(\n",
    "        merged_df[\"Accident_Percentage\"], \n",
    "        merged_df[\"Avg_Severity\"], \n",
    "        s=merged_df[\"Population\"] / 50000,  # Bubble size based on population\n",
    "        c=merged_df[\"Avg_Severity\"],  # Color based on severity\n",
    "        cmap=\"Reds\", \n",
    "        alpha=0.7,\n",
    "        edgecolors=\"k\"\n",
    "    )\n",
    "\n",
    "    # Add state labels to each point\n",
    "    for i, row in merged_df.iterrows():\n",
    "        plt.text(row[\"Accident_Percentage\"], row[\"Avg_Severity\"], row[\"State\"], fontsize=10, ha='right', va='center', color=\"black\")\n",
    "\n",
    "    # Add color bar for severity\n",
    "    cbar = plt.colorbar(scatter)\n",
    "    cbar.set_label(\"Average Severity (1=Low, 4=High)\")\n",
    "\n",
    "    # Labels and title\n",
    "    plt.xlabel(\"Accident Percentage of State Population\")\n",
    "    plt.ylabel(\"Average Severity of Accidents\")\n",
    "    plt.title(\"Scatter Plot: Accident Percentage vs. Severity per State\")\n",
    "\n",
    "    # Grid for better readability\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "\n",
    "    # Show plot\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "def plot_paginated_accident_percentage_improved(accident_data_frame, population_data_frame, states_per_page=4):\n",
    "\n",
    "    # Aggregate accident count per state and severity\n",
    "    accident_summary = accident_data_frame.groupby([\"State\", \"Severity\"]).size().unstack(fill_value=0)\n",
    "\n",
    "    # Merge with population data\n",
    "    merged_df = population_data_frame.merge(accident_summary, on=\"State\", how=\"left\").fillna(0)\n",
    "\n",
    "    # Calculate accident percentage per severity level\n",
    "    for severity in sorted(accident_summary.columns):\n",
    "        merged_df[f\"Severity_{severity}_Percentage\"] = (merged_df[severity] / merged_df[\"Population\"]) * 100\n",
    "\n",
    "    # Sort states by population (descending)\n",
    "    merged_df = merged_df.sort_values(by=\"Population\", ascending=False)\n",
    "\n",
    "    # Define severity colors\n",
    "    severity_colors = {1: \"#a6cee3\", 2: \"#1f78b4\", 3: \"#b2df8a\", 4: \"#33a02c\"}\n",
    "\n",
    "    # Paginate the states into batches\n",
    "    total_states = len(merged_df)\n",
    "    for start in range(0, total_states, states_per_page):\n",
    "        end = min(start + states_per_page, total_states)\n",
    "        subset_df = merged_df.iloc[start:end]\n",
    "\n",
    "        # Determine subplot grid size\n",
    "        num_states = len(subset_df)\n",
    "        cols = 2  # Maximum 2 columns for better readability\n",
    "        rows = int(np.ceil(num_states / cols))  # Auto-adjust row count\n",
    "\n",
    "        # Adjust figure size dynamically\n",
    "        fig_height = max(4, rows * 2.5)  # Ensure proper spacing\n",
    "        fig, axes = plt.subplots(rows, cols, figsize=(cols * 5, fig_height))\n",
    "        axes = np.array(axes).reshape(-1)  # Flatten for easy indexing\n",
    "\n",
    "        # Generate bar plots for each state\n",
    "        for idx, row in enumerate(subset_df.itertuples()):\n",
    "            state = row.State\n",
    "            population = row.Population\n",
    "            severity_percentages = [getattr(row, f\"Severity_{s}_Percentage\") for s in sorted(accident_summary.columns)]\n",
    "\n",
    "            # Plot bars\n",
    "            bars = axes[idx].bar(\n",
    "                sorted(accident_summary.columns),\n",
    "                severity_percentages,\n",
    "                color=[severity_colors[s] for s in sorted(accident_summary.columns)]\n",
    "            )\n",
    "\n",
    "            # Adjust y-axis limits to ensure no overlap\n",
    "            max_percentage = max(severity_percentages) if severity_percentages else 0\n",
    "            axes[idx].set_ylim(0, max_percentage * 1.3)  # Add space above bars\n",
    "\n",
    "            # Add title with population info, ensuring no overlap with bars\n",
    "            axes[idx].text(0.5, 1.15, f\"{state}\\nPop: {population:,}\", fontsize=10, fontweight=\"bold\",\n",
    "                           ha='center', va='top', transform=axes[idx].transAxes)\n",
    "\n",
    "            axes[idx].set_xlabel(\"Severity Level\")\n",
    "            axes[idx].set_ylabel(\"Accident % of Population\")\n",
    "            axes[idx].set_xticks(sorted(accident_summary.columns))\n",
    "\n",
    "            # Add percentage annotations on bars with dynamic vertical placement\n",
    "            for bar, percent in zip(bars, severity_percentages):\n",
    "                height_adjustment = max_percentage * 0.05  # Adjust placement dynamically\n",
    "                axes[idx].text(bar.get_x() + bar.get_width() / 2, bar.get_height() + height_adjustment,\n",
    "                               f\"{percent:.2f}%\", ha='center', va='bottom', fontsize=8, color=\"black\")\n",
    "\n",
    "        # Hide any unused subplots (if fewer states than grid size)\n",
    "        for i in range(idx + 1, len(axes)):\n",
    "            fig.delaxes(axes[i])\n",
    "  \n",
    "        # Show the current page of subplots\n",
    "        plt.suptitle(f\"Accident Percentage by Severity Level per State (Sorted by Population)\\nPage {start // states_per_page + 1}\", \n",
    "                     fontsize=14, fontweight=\"bold\")\n",
    "        plt.tight_layout()  # Ensures spacing is adjusted properly\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_file_frame = process_large_csv(raw_file_path, chunk_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " *****************  Starting Data Cleanup  **************\n",
      "Shape:  4325632\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 4325632 entries, 3402762 to 7728393\n",
      "Data columns (total 27 columns):\n",
      " #   Column             Dtype  \n",
      "---  ------             -----  \n",
      " 0   Severity           int64  \n",
      " 1   Start_Time         object \n",
      " 2   End_Time           object \n",
      " 3   Start_Lat          float64\n",
      " 4   Start_Lng          float64\n",
      " 5   Street             object \n",
      " 6   City               object \n",
      " 7   County             object \n",
      " 8   State              object \n",
      " 9   Zipcode            object \n",
      " 10  Country            object \n",
      " 11  Timezone           object \n",
      " 12  Weather_Timestamp  object \n",
      " 13  Temperature(F)     float64\n",
      " 14  Wind_Chill(F)      float64\n",
      " 15  Humidity(%)        float64\n",
      " 16  Visibility(mi)     float64\n",
      " 17  Wind_Speed(mph)    float64\n",
      " 18  Weather_Condition  object \n",
      " 19  Bump               bool   \n",
      " 20  Crossing           bool   \n",
      " 21  Junction           bool   \n",
      " 22  Railway            bool   \n",
      " 23  Roundabout         bool   \n",
      " 24  Station            bool   \n",
      " 25  Stop               bool   \n",
      " 26  Traffic_Signal     bool   \n",
      "dtypes: bool(8), float64(7), int64(1), object(11)\n",
      "memory usage: 693.0+ MB\n",
      "\n",
      " Dataframe after droping roes: \n",
      " None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 4325632 entries, 3402762 to 7728393\n",
      "Data columns (total 26 columns):\n",
      " #   Column             Dtype  \n",
      "---  ------             -----  \n",
      " 0   Severity           int64  \n",
      " 1   Start_Time         object \n",
      " 2   End_Time           object \n",
      " 3   Start_Lat          float64\n",
      " 4   Start_Lng          float64\n",
      " 5   Street             object \n",
      " 6   City               object \n",
      " 7   County             object \n",
      " 8   State              object \n",
      " 9   Zipcode            object \n",
      " 10  Country            object \n",
      " 11  Timezone           object \n",
      " 12  Weather_Timestamp  object \n",
      " 13  Temperature(F)     float64\n",
      " 14  Humidity(%)        float64\n",
      " 15  Visibility(mi)     float64\n",
      " 16  Wind_Speed(mph)    float64\n",
      " 17  Weather_Condition  object \n",
      " 18  Bump               bool   \n",
      " 19  Crossing           bool   \n",
      " 20  Junction           bool   \n",
      " 21  Railway            bool   \n",
      " 22  Roundabout         bool   \n",
      " 23  Station            bool   \n",
      " 24  Stop               bool   \n",
      " 25  Traffic_Signal     bool   \n",
      "dtypes: bool(8), float64(6), int64(1), object(11)\n",
      "memory usage: 660.0+ MB\n",
      "\n",
      " Dataframe after droping roes: \n",
      " None\n",
      "\n",
      " Shape: 4325632\n",
      " \n",
      " Shape: 4236645\n",
      "Is Null Sum:\n",
      "  Severity                  0\n",
      "Start_Time                0\n",
      "End_Time                  0\n",
      "Start_Lat                 0\n",
      "Start_Lng                 0\n",
      "Street                    0\n",
      "City                      0\n",
      "County                    0\n",
      "State                     0\n",
      "Zipcode                   0\n",
      "Country                   0\n",
      "Timezone                  0\n",
      "Weather_Timestamp         0\n",
      "Temperature(F)        26223\n",
      "Humidity(%)           32582\n",
      "Visibility(mi)        31555\n",
      "Wind_Speed(mph)      127846\n",
      "Weather_Condition     28552\n",
      "Bump                      0\n",
      "Crossing                  0\n",
      "Junction                  0\n",
      "Railway                   0\n",
      "Roundabout                0\n",
      "Station                   0\n",
      "Stop                      0\n",
      "Traffic_Signal            0\n",
      "dtype: int64\n",
      "(4236645, 26)\n",
      "(4065577, 26)\n",
      "Saved Cleaned all state Data  \n",
      " \n"
     ]
    }
   ],
   "source": [
    "print(\" *****************  Starting Data Cleanup  **************\")\n",
    "raw_data_file_frame = raw_data_file_frame[raw_data_file_frame[\"Source\"].str.lower() == \"source1\"]\n",
    "raw_data_file_frame = drop_columns(raw_data_file_frame)\n",
    "print('Shape: ',raw_data_file_frame.shape[0])\n",
    "print(\"\\n Dataframe after droping roes: \\n\",raw_data_file_frame.info())\n",
    "\n",
    "missing_data = pd.DataFrame(raw_data_file_frame.isnull().sum()).reset_index()\n",
    "missing_data.columns = ['Feature', 'Missing_Percent(%)']\n",
    "missing_data['Missing_Percent(%)'] = missing_data['Missing_Percent(%)'].apply(lambda x: x / raw_data_file_frame.shape[0] * 100)\n",
    "missing_data.loc[missing_data['Missing_Percent(%)']>0,:]\n",
    "\n",
    "raw_data_file_frame = raw_data_file_frame.drop(['Wind_Chill(F)'], axis=1)\n",
    "print(\"\\n Dataframe after droping roes: \\n\",raw_data_file_frame.info())\n",
    "\n",
    "print(\"\\n Shape:\", raw_data_file_frame.shape[0])\n",
    "\n",
    "raw_data_file_frame = raw_data_file_frame.dropna(subset=['City','Zipcode','Street',\n",
    "                       'Zipcode','County','Timezone','Weather_Timestamp'])\n",
    "print(\" \\n Shape:\", raw_data_file_frame.shape[0])\n",
    "print(\"Is Null Sum:\\n \",raw_data_file_frame.isnull().sum())\n",
    "print(raw_data_file_frame.shape)\n",
    "raw_data_file_frame.dropna(inplace = True)\n",
    "print(raw_data_file_frame.shape)\n",
    "#split_and_save_by_state(raw_data_file_frame)\n",
    "save_cleaned_all_state_data(raw_data_file_frame)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 4065577 entries, 3402762 to 7728393\n",
      "Data columns (total 26 columns):\n",
      " #   Column             Dtype  \n",
      "---  ------             -----  \n",
      " 0   Severity           int64  \n",
      " 1   Start_Time         object \n",
      " 2   End_Time           object \n",
      " 3   Start_Lat          float64\n",
      " 4   Start_Lng          float64\n",
      " 5   Street             object \n",
      " 6   City               object \n",
      " 7   County             object \n",
      " 8   State              object \n",
      " 9   Zipcode            object \n",
      " 10  Country            object \n",
      " 11  Timezone           object \n",
      " 12  Weather_Timestamp  object \n",
      " 13  Temperature(F)     float64\n",
      " 14  Humidity(%)        float64\n",
      " 15  Visibility(mi)     float64\n",
      " 16  Wind_Speed(mph)    float64\n",
      " 17  Weather_Condition  object \n",
      " 18  Bump               bool   \n",
      " 19  Crossing           bool   \n",
      " 20  Junction           bool   \n",
      " 21  Railway            bool   \n",
      " 22  Roundabout         bool   \n",
      " 23  Station            bool   \n",
      " 24  Stop               bool   \n",
      " 25  Traffic_Signal     bool   \n",
      "dtypes: bool(8), float64(6), int64(1), object(11)\n",
      "memory usage: 620.4+ MB\n",
      "\n",
      " Dataframe after adding new columns roes: \n",
      " None\n",
      "         Severity           Start_Time             End_Time  Start_Lat  \\\n",
      "3402762         3  2016-02-08 00:37:08  2016-02-08 06:37:08  40.108910   \n",
      "3402767         3  2016-02-08 07:53:43  2016-02-08 13:53:43  39.172393   \n",
      "3402768         2  2016-02-08 08:16:57  2016-02-08 14:16:57  39.063240   \n",
      "3402769         2  2016-02-08 08:16:57  2016-02-08 14:16:57  39.067080   \n",
      "3402770         2  2016-02-08 08:15:41  2016-02-08 14:15:41  39.775650   \n",
      "3402771         2  2016-02-08 11:51:46  2016-02-08 17:51:46  41.375310   \n",
      "3402772         2  2016-02-08 14:19:57  2016-02-08 20:19:57  40.702247   \n",
      "3402773         2  2016-02-08 15:16:43  2016-02-08 21:16:43  40.109310   \n",
      "3402774         2  2016-02-08 15:43:50  2016-02-08 21:43:50  39.192880   \n",
      "3402775         2  2016-02-08 16:50:57  2016-02-08 22:50:57  39.138770   \n",
      "\n",
      "         Start_Lng           Street          City      County State Zipcode  \\\n",
      "3402762 -83.092860      Outerbelt E        Dublin    Franklin    OH   43017   \n",
      "3402767 -84.492792           I-75 S    Cincinnati    Hamilton    OH   45217   \n",
      "3402768 -84.032430   State Route 32  Williamsburg    Clermont    OH   45176   \n",
      "3402769 -84.058550   State Route 32       Batavia    Clermont    OH   45103   \n",
      "3402770 -84.186030           I-75 S        Dayton  Montgomery    OH   45404   \n",
      "3402771 -81.820170           I-71 S     Cleveland    Cuyahoga    OH   44130   \n",
      "3402772 -84.075887    E Hanthorn Rd          Lima       Allen    OH   45806   \n",
      "3402773 -82.968490      Outerbelt W   Westerville    Franklin    OH   43081   \n",
      "3402774 -84.477230           I-75 N    Cincinnati    Hamilton    OH   45216   \n",
      "3402775 -84.533940        Hopple St    Cincinnati    Hamilton    OH   45225   \n",
      "\n",
      "         ... Wind_Speed(mph) Weather_Condition   Bump  Crossing  Junction  \\\n",
      "3402762  ...            10.4        Light Rain  False     False     False   \n",
      "3402767  ...            10.4        Light Rain  False     False     False   \n",
      "3402768  ...             8.1          Overcast  False     False     False   \n",
      "3402769  ...             8.1          Overcast  False     False     False   \n",
      "3402770  ...             2.3     Mostly Cloudy  False     False     False   \n",
      "3402771  ...             3.5              Snow  False     False      True   \n",
      "3402772  ...            11.5          Overcast  False     False     False   \n",
      "3402773  ...             3.5              Snow  False     False     False   \n",
      "3402774  ...             4.6        Light Snow  False     False     False   \n",
      "3402775  ...             3.5          Overcast  False      True     False   \n",
      "\n",
      "         Railway  Roundabout Station   Stop  Traffic_Signal  \n",
      "3402762    False       False   False  False           False  \n",
      "3402767    False       False   False  False           False  \n",
      "3402768    False       False   False  False            True  \n",
      "3402769    False       False   False  False           False  \n",
      "3402770    False       False   False  False           False  \n",
      "3402771    False       False   False  False           False  \n",
      "3402772    False       False   False  False           False  \n",
      "3402773    False       False   False  False           False  \n",
      "3402774    False       False   False  False           False  \n",
      "3402775    False       False   False  False            True  \n",
      "\n",
      "[10 rows x 26 columns]\n"
     ]
    }
   ],
   "source": [
    "cleaned_data_file_frame = raw_data_file_frame\n",
    "print(\"\\n Dataframe after adding new columns roes: \\n\",cleaned_data_file_frame.info())\n",
    "print(cleaned_data_file_frame.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "fl_data_frame = process_large_csv(fl_data_file, chunk_size)\n",
    "ca_data_frame = process_large_csv(ca_data_file, chunk_size)\n",
    "va_data_frame = process_large_csv(va_data_file, chunk_size)\n",
    "ny_data_frame = process_large_csv(ny_data_file, chunk_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
